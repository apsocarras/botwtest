---
title: "Scraping Data with Tuber"
output: html_document
date: '2022-07-06'
---

```{r import}
library(tuber)
library(here)
library(writexl)

# library(tidyverse) - Tidyverse Packages: 
library(dplyr)
library(tidyr)
library(stringr)
library(readxl)
library(tibble)
library(lubridate)
library(purrr)
```

In this project we will use the following R & Python packages to obtain video metadata and transcripts from RedLetterMedia's [*Best of the Worst*](https://www.youtube.com/playlist?list=PLJ_TJFLc25JR3VZ7Xe-cmt4k3bMKBZ5Tm) YouTube series: 

|   tuber -- obtain video metadata from playlist
|   YoutubeTranscriptApi -- obtain video transcripts

# Obtaining Video Metadata:

```{r API authorization}

client_id <- "750567258427-tf07hsleu2ov7sp6fcp538cg7itkg7ii.apps.googleusercontent.com"
client_secret <- "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"

yt_oauth(
  app_id = client_id, 
  app_secret = client_secret,
  token = '')

play_id <- stringr::str_split(
  string = "https://www.youtube.com/playlist?list=PLJ_TJFLc25JR3VZ7Xe-cmt4k3bMKBZ5Tm",
  pattern = "=",
  n = 2, simplify = TRUE)[,2]

```

For `r tuber::get_playlist_items()`, Google limits our API calls to accessing a maximum of 50 videos at a time. We can circumvent this for the first call by specifying the 'pageToken' value for the first half of the playlist (i.e. the first 73 uploads):

```{r get_playlist_items}
page1 <- get_playlist_items(filter = c(playlist_id = play_id),
           part = "snippet",
           max_results = 100, # given to override the default value of 50
           page_token = "EAAaBlBUOkNESQ", 
           simplify = TRUE)    

page2 <- get_playlist_items(filter =
          c(playlist_id = play_id),
          part = "snippet",
          max_results = 100, 
          simplify = FALSE)
```

For some reason, the [YouTubeApi](https://developers.google.com/youtube/v3/guides/implementation/pagination) allows queries to access tokens for the next page (`nextPageToken`) or previous page (`prevPageToken`) of a given set of results, but not the current page. Fortunately, we can still see our desired page token "EAAaBlBUOkNESQ" in the unsimplified output above (`r page2$nextPageToken`). Note that it's listed under `nextPageToken`: the first page in a YouTube [`playlistItem`](https://developers.google.com/youtube/v3/docs/playlistItems) object is the most recent set of uploads rather than the earliest.

Next we process and combine both pages of our results into a single dataframe, showing cleaning methods for both the simplified and unsimplified output of `r tuber::get_playlist_items()`:

```{r }
# Simplified Output:
page1.df <- page1 %>% 
  select(date = snippet.publishedAt,
    title = snippet.title,
    description = snippet.description,
    video_id = snippet.resourceId.videoId) %>% 
    rowid_to_column("pl_order") %>% 
    mutate(pl_order = rev(as.integer(pl_order))) %>%
    arrange(pl_order)

# Unsimplified Output:
page2.ls <- list()

for (j in c(1:50)) {
  info <- page2[["items"]][[j]]$snippet[c("publishedAt", "title", "description", "resourceId")]
  page2.ls[[j]] <- info
}

page2.df <- as_tibble(page2.ls, .name_repair = "universal") %>%
  rename_with(~ gsub("...", "",.x)) %>%
  mutate(categories = c("date","title","description","resource")) %>%
  select(categories,1:50) %>%
  pivot_longer(names_to = "pl_order", cols = c(2:51)) %>%
  mutate(pl_order = rev(as.integer(pl_order)) + nrow(scrape1.df)) %>%
  pivot_wider(names_from = categories) %>%
  unnest(cols = 2:5) %>%
  filter(!grepl("youtube#", resource)) %>%
  mutate(video_id = as.character(resource), .keep = c("unused"),
         title = replace(title, title == "Deleted video",
            "Best of the Worst: Diamond Cobra vs the White Fox")) %>% # removed due to copyright strike
  arrange(pl_order)

```

The original playlist contains video extras which are not actual episodes in *Best of the Worst.* We will want to distinguish these two groups of videos for our analysis later on. Below is an easy (if inelegant) method of adding episode numbers to only those videos in the series itself:

```{r}
episodes <- full_join(page1.df, page2.df) %>% 
  filter(grepl("Best of the Worst(:| Episode| Spotlight)",title)) %>% 
  mutate(ep_num = as.numeric(row_number())) 

non_episodes <- full_join(page1.df, page2.df) %>%
    filter(!grepl("Best of the Worst(:| Episode| Spotlight)",title))

botw.df <- full_join(non_episodes, episodes) %>% 
  arrange(pl_order) %>%
  select(pl_order,ep_num, title, date, description, video_id)
```

There are two episodes we need to modify separately:
|   Episode 27 was uploaded in two separate parts 
|   Episode 63 contains an additional "surprise" episode

We will code these special cases as "half" episodes:

```{r episode 27}
botw.df <- botw.df %>% mutate(ep_num = case_when(ep_num == 28 ~ 27.5,
                            ep_num > 28 ~ ep_num - 1,
                            TRUE ~ ep_num)) # leaves ep_nums 1:27 unchanged

ep_63.5 <- botw.df %>% filter(ep_num == 63) %>% 
            mutate(ep_num = 63.5, title = "Best of the Worst Spotlight: Partners")

botw.df <- rbind(botw.df,ep_63.5) %>% arrange(pl_order)

```


For many of the videos in our playlist, we can use the title and description to identify the associated subseries and holiday (where applicable). Unfortunately, we have to code the same information manually for the rest: 

```{r}
# Manual codings -- these episodes lacked the relevant information in their titles and videos
spotlight_other <- c(27,27.5,56,59,82,92,95)
main_other <- c(1,18,73,86,105)
random_other <- c(36,61,73,90,107,108)


botw.df <- botw.df %>%
  mutate (subseries = case_when(
            is.na(ep_num) ~ "extras",
            grepl(",", title) ~ "main", # only main series episodes have multiple movies in their titles
            grepl("[Ww]heel",description) | grepl("[Ww]heel", title) ~ "wheel",
            grepl("[Ss]pine", description) | grepl("[Ss]pine", title) ~ "bl_spine",
            grepl("([Pp]linketto | ball)", description) | grepl("[Pp]linketto", title) ~ "plinketto",
            grepl("[Ss]potlight", description) | grepl("[Ss]potlight", title) ~ "spotlight",
            ep_num %in% spotlight_other ~ "spotlight",
            ep_num %in% main_other ~ "main",
            ep_num %in% random_other ~ "random_other",
            ep_num == 96 ~ "bl_spine"),
        holiday = case_when(
          grepl("(Christmas| Holiday |-mas)", title) ~ "christmas",
          grepl("Halloween", description) & subseries != "wheel" ~ "halloween",
          grepl("Hollywood Cop", title) ~ "tums festival", # watch the episode...
          TRUE ~ "none"),
          date = as_datetime(date))


```


## Fan-supplied Metatdata

The [RedLetterMedia subreddit](https://www.reddit.com/r/RedLetterMedia/) has a fan-updated spreadsheet containing more information on all the episodes in the series (featured panelists, winning movies and videos, etc.). After filling in some missing datapoints manually, we process the data set as follows: 

```{r}
# First, a helper function for later on 
get_duration <- function(time) { 
  time %>%
    stringr::str_split(" ") %>%
    purrr::map_chr(2) %>%
    lubridate::hms()
}

fansheet1 <- read_excel(here("data/reddit_sheet.xlsx"), sheet =1) %>%
  purrr::discard(~all(is.na(.))) %>% # Drops NA whitespace columns 
  select(-24)

fansheet2 <- read_excel(here("data/reddit_sheet.xlsx"), sheet = 2) %>% 
  select(-3, -11) 

# Replacing some of the "✓"/"X" columns with more informative values
## TO-DO: Replace with apply() family functions 
for (i in c(8:15)) {
  fansheet1[[i]] <- case_when(grepl("✓", fansheet1[[i]]) ~ names(fansheet1)[[i]],
                               grepl("X", fansheet1[[i]]) ~ "",
                               TRUE ~ fansheet1[[i]]) } # TRUE is the else condition
for (i in c(3:8)) {
  fansheet2[[i]] <- case_when(grepl("✓", fansheet2[[i]]) ~ names(fansheet2)[[i]],
                               grepl("X", fansheet2[[i]]) ~ "",
                               TRUE ~ fansheet2[[i]]) }

fansheet <- full_join(fansheet1, fansheet2) %>% 
  rename_all(tolower) %>%
  rename(theme = `theme / gimmick`, best = `best of the worst`,
         worst = `worst of the worst`, destruction_method = `method of destruction`,
         date = `date released`) %>%
  mutate(`first film` = if_else(is.na(`first film`), film, `first film`),
         theme = if_else(is.na(theme), "spotlight", theme)) %>%
  unite("films", 2:4, sep = ", ", remove = TRUE, na.rm = TRUE) %>%
  unite("panelists", mike:guests, sep = " ", remove = TRUE, na.rm = TRUE) %>%
  mutate(panelists = str_squish(str_remove_all(panelists, "from Canada|&")), 
         duration = get_duration(length),
         unanimous_win = case_when(grepl("✓", `unanimous win`) ~ "yes",
                               grepl("X", `unanimous win`) ~ "no",
                               TRUE ~ `unanimous win`)) %>% # NA
  select(episode, date, duration, films:best, unanimous_win, worst:destruction_method,
         -`youtube links`, -`# gimmik`) %>% 
  arrange(episode)
```

You might have noticed that `r fansheet$date` doesn't always agree with the date column as generated by Tuber (`r botw.df$date`). This is because `r get_playlist_items()` gave us the date on which the video was added to the series *playlist*, not its YouTube upload date. We can use `r get_video_details()` to access the upload date of each video directly and use these to check our original dates in `botw.df`: 


```{r}
upload_dates <- character() 

for (id in botw.df$video_id) {
  
  up_date = get_video_details(video_id = id,part = "snippet")$items[[1]]$snippet$publishedAt
  
  if(is.null(up_date)) {up_date = "date missing"} 

  upload_dates = c(upload_dates, up_date)
  
}

date_check <- botw.df %>% 
  select(pl_order, ep_num) %>% 
  mutate(upload_date = as_datetime(upload_dates))

```

But not all of these are the correct upload dates, either: two are out of order from when videos were re-uploaded, and one is missing since the video was removed. For these three, we'll need to use the fan-supplied dates: 

```{r}
date_check %>% 
  filter(upload_date > lead(upload_date) | is.na(upload_date)) # Episodes 2, 26, 92

for (ep in c(2,26,92)) {
  
date_check["upload_date"][date_check["ep_num"] == ep & !is.na(date_check$ep_num)] 
  <- fansheet["date"][fansheet["episode"] == ep]

}

botw.df <- left_join(botw.df, date_check) %>% 
  select(pl_order:title, upload_date, description:holiday)

# writexl::write_xlsx(botw.df,here("data/botw.xlsx"))
``` 

Instead of just fixing the dates in `botw.df`, let's attach all the other columns from `fansheet` as well. Before we do that, we'll have to edit the fan data set to fit with the changes we made to the "half" episodes earlier:

```{r}

fansheet[fansheet["episode"] == 27,"duration"] <- ms("42M 54S")
# can leave date alone -- we're only gonna use `botw.df$upload_date` like before


ep_27.5 <- fansheet[fansheet["episode"] == 27,] %>%
  mutate(duration = ms("43M 40S"),
         episode = 27.5) 

fansheet <- rbind(fansheet, ep_27.5) %>% arrange(episode) 

fansheet[grep("Blackstreetboyz", fansheet$films),"duration"] <- ms("31M 45S")
fansheet[grep("Partners", fansheet$films), "duration"] <- ms("38M 0S")
fansheet[grep("Partners", fansheet$films), "episode"] <- 63.5


botw.df2 <- left_join(botw.df, fansheet, by = c("ep_num" = "episode")) %>%
  select(pl_order, ep_num, upload_date, title, description, subseries, holiday, theme, duration, films, best, unanimous_win, worst, destruction_method,`non-reviewed / extra videos`, panelists, editor, video_id) 

```

# Next Steps: 

In the next phase of this project, we'll use the Python `YTApi` package to obtain transcripts of the episodes in the series, isolate the discussion portions, and match them to the movies being discussed. 

